{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchdata.datapipes.map import SequenceWrapper, Batcher\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import wandb\n",
    "import einops\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from utils import PositionalEncoding, bincount_along_dim, get_returns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*train_dataloader yielded None.*\")\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MastermindEnvironment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_pins,\n",
    "        n_colors,\n",
    "        n_turns,\n",
    "        batch_size=1,\n",
    "        device=torch.device(\"cpu\"),\n",
    "    ):\n",
    "        self.n_pins = n_pins\n",
    "        self.n_colors = n_colors\n",
    "        self.n_turns = n_turns\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.turn = None\n",
    "        self.code = None\n",
    "        self.guesses = None\n",
    "        self.scores_full = None\n",
    "        self.scores_color = None\n",
    "        _ = self.reset()\n",
    "\n",
    "    @property\n",
    "    def scores(self):\n",
    "        return self.scores_full, self.scores_color\n",
    "\n",
    "    def reset(self):\n",
    "        self.turn = 1\n",
    "        self.code = torch.randint(\n",
    "            low=0,\n",
    "            high=self.n_colors,\n",
    "            size=(self.batch_size, self.n_pins),\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.guesses = torch.zeros(\n",
    "            (0, self.batch_size, self.n_pins),\n",
    "            dtype=torch.long,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.scores_full = torch.zeros(\n",
    "            (1, self.batch_size),\n",
    "            dtype=torch.long,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.scores_color = torch.zeros(\n",
    "            (1, self.batch_size),\n",
    "            dtype=torch.long,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return self.guesses, self.scores\n",
    "\n",
    "    def step(self, guess):\n",
    "        if isinstance(guess, list):\n",
    "            guess = torch.tensor(guess, dtype=torch.long, device=self.device)\n",
    "        assert self.turn <= self.n_turns\n",
    "        assert guess.size() == (self.batch_size, self.n_pins)\n",
    "\n",
    "        self.guesses = torch.cat((self.guesses, guess.unsqueeze(0)), dim=0)\n",
    "\n",
    "        current_score_full = (self.code == guess).sum(dim=1)\n",
    "        self.scores_full = torch.cat(\n",
    "            (self.scores_full, current_score_full.unsqueeze(0)), dim=0\n",
    "        )\n",
    "\n",
    "        code_color_counts = bincount_along_dim(self.code, max_val=self.n_colors, dim=1)\n",
    "        guess_color_counts = bincount_along_dim(guess, max_val=self.n_colors, dim=1)\n",
    "        num_correct_colors = torch.min(code_color_counts, guess_color_counts).sum(dim=1)\n",
    "        current_score_color = num_correct_colors - current_score_full\n",
    "        self.scores_color = torch.cat(\n",
    "            (self.scores_color, current_score_color.unsqueeze(0)), dim=0\n",
    "        )\n",
    "\n",
    "        done = (self.scores_full == self.n_pins).any(dim=0)\n",
    "        reward = self.reward_func(done)\n",
    "\n",
    "        self.turn += 1\n",
    "\n",
    "        return self.guesses, self.scores, reward, done\n",
    "\n",
    "    def reward_func(self, done):\n",
    "        # -1 for each turn that doesn't solve the code, -3.0 more for last turn if not solved\n",
    "        reward = torch.where(done, 0.0, -1.0)\n",
    "        if self.turn == self.n_turns:\n",
    "            reward[~done] -= 3.0\n",
    "        return reward\n",
    "\n",
    "    def visualize(self, show_answer=False, num_games=None, scale=0.5):\n",
    "        if num_games is None:\n",
    "            num_games = self.batch_size\n",
    "        width, height = (1.25 * scale * (self.n_pins + 1), scale * (self.n_turns + 1))\n",
    "        fig, axes = plt.subplots(1, num_games, figsize=(width * num_games, height))\n",
    "        if num_games == 1:\n",
    "            axes = [axes]\n",
    "        for i in range(num_games):\n",
    "            ax, guesses, scores_full, scores_color, code = (\n",
    "                axes[i],\n",
    "                self.guesses[:, i].cpu().numpy(),\n",
    "                self.scores_full[1:, i].cpu().numpy(),\n",
    "                self.scores_color[1:, i].cpu().numpy(),\n",
    "                self.code[i].cpu().numpy(),\n",
    "            )\n",
    "            # Guesses\n",
    "            sns.scatterplot(\n",
    "                x=list(range(1, self.n_pins + 1)) * (self.turn - 1),\n",
    "                y=sum([[t] * self.n_pins for t in range(1, self.turn)], []),\n",
    "                hue=[str(g) for g in guesses.flatten()],\n",
    "                hue_order=[str(c) for c in range(self.n_colors)],\n",
    "                s=scale * 200,\n",
    "                ax=ax,\n",
    "            )\n",
    "            # Answer\n",
    "            if show_answer:\n",
    "                sns.scatterplot(\n",
    "                    x=range(1, self.n_pins + 1),\n",
    "                    y=[self.n_turns + 1.5] * self.n_pins,\n",
    "                    hue=[str(c) for c in code],\n",
    "                    hue_order=[str(c) for c in range(self.n_colors)],\n",
    "                    s=scale * 200,\n",
    "                    ax=ax,\n",
    "                )\n",
    "            # Bounds\n",
    "            ax.set(\n",
    "                xlim=(0, self.n_pins + 1),\n",
    "                ylim=(0, self.n_turns + 1 + show_answer * 1.5),\n",
    "            )\n",
    "            if show_answer:\n",
    "                ax.set_yticks(\n",
    "                    list(range(1, self.n_turns + 1)) + [self.n_turns + 1.5],\n",
    "                    labels=list(range(1, self.n_turns + 1)) + [\"Answer\"],\n",
    "                )\n",
    "            else:\n",
    "                ax.set_yticks(range(1, self.n_turns + 1))\n",
    "            # Scores\n",
    "            y_score = ax.twinx()\n",
    "            y_score.set_ylim(ax.get_ylim())\n",
    "            y_score.set_yticks(\n",
    "                range(1, self.turn),\n",
    "                labels=[f\"{f} / {c}\" for f, c in zip(scores_full, scores_color)],\n",
    "            )\n",
    "            # Other aesthetics\n",
    "            ax.set(xticks=[])\n",
    "            ax.legend().remove()\n",
    "        fig.tight_layout()\n",
    "        return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MastermindPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_pins,\n",
    "        n_colors,\n",
    "        n_turns,\n",
    "        hidden_dim=128,\n",
    "        n_heads=2,\n",
    "        n_layers=2,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % n_pins == 0\n",
    "\n",
    "        self.n_pins = n_pins\n",
    "        self.n_colors = n_colors\n",
    "        self.n_turns = n_turns\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.guess_embedding = nn.Linear(n_colors, int(hidden_dim / n_pins))\n",
    "        self.score_embedding = nn.Linear(2 * (n_pins + 1), hidden_dim)\n",
    "\n",
    "        self.pe = PositionalEncoding(hidden_dim, max_len=n_turns * 2 - 1, dropout=0.0)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim, nhead=n_heads, dropout=dropout\n",
    "            ),\n",
    "            num_layers=n_layers,\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Linear(int(hidden_dim / n_pins), n_colors)\n",
    "\n",
    "    def forward(self, guesses, scores, exploration_eps=0.1):\n",
    "        \"\"\"Action policy\n",
    "\n",
    "        Args:\n",
    "            guesses (torch.LongTensor): (n_guesses, bs, n_pins) with values in (0, n_colors - 1)\n",
    "            scores (tuple[torch.LongTensor, torch.LongTensor]): Each is (n_guesses + 1, bs) with values in (0, n_pins)\n",
    "            exploration_eps (float, optional): Probability of random exploratory action. Defaults to 0.1.\n",
    "\n",
    "        Returns:\n",
    "            (tuple[torch.LongTensor, torch.FloatTensor]): Tuple of the action (bs, n_pins) and it's probability under the policy (bs)\n",
    "        \"\"\"\n",
    "        assert scores[0].size(0) == guesses.size(0) + 1\n",
    "        n_guesses, bs, _ = guesses.size()\n",
    "        scores_full, scores_color = scores\n",
    "\n",
    "        # Embed guesses and scores\n",
    "        guesses = F.one_hot(guesses, num_classes=self.n_colors).float()\n",
    "        guesses = self.guess_embedding(guesses)\n",
    "        guesses = guesses.view(n_guesses, bs, self.hidden_dim)\n",
    "        scores_full = F.one_hot(scores_full, num_classes=self.n_pins + 1).float()\n",
    "        scores_color = F.one_hot(scores_color, num_classes=self.n_pins + 1).float()\n",
    "        scores = torch.cat([scores_full, scores_color], dim=-1)\n",
    "        scores = self.score_embedding(scores)\n",
    "\n",
    "        # Build state as a sequence of interleaved guesses and scores\n",
    "        state = einops.rearrange(\n",
    "            [scores[:-1], guesses], \"k n_guesses bs d -> (n_guesses k) bs d\"\n",
    "        )\n",
    "        state = torch.cat([state, scores[-1:]], dim=0)\n",
    "        state = self.pe(state)\n",
    "\n",
    "        # Run transformer and get the representation of the action from the last token\n",
    "        representation = self.transformer(state)[-1]\n",
    "        representation = representation.view(bs, self.n_pins, -1)\n",
    "\n",
    "        # Sample an action from the policy and compute its probability\n",
    "        p = F.softmax(\n",
    "            self.policy_head(representation), dim=-1\n",
    "        )  # (bs, n_pins, n_colors)\n",
    "        if random.random() < exploration_eps:\n",
    "            p_action = torch.ones_like(p) / self.n_colors\n",
    "        else:\n",
    "            p_action = p\n",
    "        next_guess = torch.multinomial(\n",
    "            p_action.view(-1, self.n_colors),\n",
    "            num_samples=1,\n",
    "        ).view(bs, self.n_pins)\n",
    "        p_next_guess = (\n",
    "            p.gather(dim=-1, index=next_guess.unsqueeze(-1)).squeeze(-1).prod(dim=-1)\n",
    "        )\n",
    "\n",
    "        return next_guess, p_next_guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MastermindRL(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_pins=4,\n",
    "        n_colors=8,\n",
    "        n_turns=12,\n",
    "        hidden_dim=128,\n",
    "        n_heads=2,\n",
    "        n_layers=2,\n",
    "        dropout=0.2,\n",
    "        batch_size=32,\n",
    "        val_episodes=100,\n",
    "        train_episodes=1000,\n",
    "        lr=1e-3,\n",
    "        exploration_eps=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.env = MastermindEnvironment(\n",
    "            n_pins=n_pins,\n",
    "            n_colors=n_colors,\n",
    "            n_turns=n_turns,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        self.policy = MastermindPolicy(\n",
    "            n_pins=n_pins,\n",
    "            n_colors=n_colors,\n",
    "            n_turns=n_turns,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_heads=n_heads,\n",
    "            n_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        p, reward, done = self.unroll_episode()\n",
    "        loss = self.loss_func(p, reward, done)\n",
    "        num_turns = (~done).sum(dim=0).float().mean()\n",
    "        self.log(\"train/policy_loss\", loss, on_epoch=True, on_step=False)\n",
    "        self.log(\n",
    "            \"train/num_turns\", num_turns, on_epoch=True, on_step=False, prog_bar=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        p, reward, done = self.unroll_episode()\n",
    "        loss = self.loss_func(p, reward, done)\n",
    "        num_turns = (~done).sum(dim=0).float().mean()\n",
    "        self.log(\"val/policy_loss\", loss)\n",
    "        self.log(\"val/num_turns\", num_turns, prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def unroll_episode(self):\n",
    "        exploration_eps = self.hparams.exploration_eps if self.training else 0.0\n",
    "        guesses, scores = self.env.reset()\n",
    "        p_guesses, rewards, dones = [], [], []\n",
    "        dones.append(\n",
    "            torch.zeros(self.hparams.batch_size, dtype=torch.bool, device=self.device)\n",
    "        )\n",
    "\n",
    "        while not dones[-1].all() and self.env.turn <= self.hparams.n_turns:\n",
    "            next_guess, p_next_guess = self.policy(\n",
    "                guesses, scores, exploration_eps=exploration_eps\n",
    "            )\n",
    "            guesses, scores, reward, done = self.env.step(next_guess)\n",
    "            p_guesses.append(p_next_guess)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "        p_guesses, rewards, dones = (\n",
    "            torch.stack(p_guesses),\n",
    "            torch.stack(rewards),\n",
    "            torch.stack(dones),\n",
    "        )\n",
    "\n",
    "        return p_guesses, rewards, dones\n",
    "\n",
    "    def loss_func(self, p, reward, done):\n",
    "        # REINFORCE loss\n",
    "        returns = get_returns(reward)\n",
    "        loss = -p.log() * returns\n",
    "        loss = torch.where(\n",
    "            ~done[:-1], loss, 0\n",
    "        )  # Mask out transitions from completed trajectories\n",
    "        return loss.sum(dim=0).mean()  # Sum across trajectories, mean across batch\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        self.env.device = self.device\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            torch.zeros(\n",
    "                self.hparams.train_episodes // self.hparams.batch_size,\n",
    "                self.hparams.batch_size,\n",
    "            ).bool(),\n",
    "            batch_size=None,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            torch.zeros(\n",
    "                self.hparams.val_episodes // self.hparams.batch_size,\n",
    "                self.hparams.batch_size,\n",
    "            ).bool(),\n",
    "            batch_size=None,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.policy.parameters(), lr=self.hparams.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = MastermindRL(n_pins=2, n_colors=3)\n",
    "\n",
    "wandb.finish()\n",
    "trainer = Trainer(\n",
    "    max_epochs=1000,\n",
    "    logger=WandbLogger(\n",
    "        project=\"Mastermind\",\n",
    "        entity=\"ericelmoznino\",\n",
    "        save_dir=\"trained_models\",\n",
    "    ),\n",
    "    # callbacks=[\n",
    "    #     ModelCheckpoint(monitor=\"val/num_turns\"),\n",
    "    #     EarlyStopping(monitor=\"val/num_turns\", patience=30),\n",
    "    # ],\n",
    ")\n",
    "trainer.fit(model=task)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfn-attractors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
